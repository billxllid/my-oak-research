---
description: LLM 服务网关设计规范（统一接口/模型路由/缓存/限流/监控/成本控制）
globs:
  - "packages/agents/**"
  - "app/api/ai/**"
alwaysApply: true
---

# Oak Research - LLM 服务网关设计规范

本规范定义统一的 LLM 调用网关，支持多模型路由、请求缓存、速率限制、成本监控与故障转移，为上层 AI 功能提供稳定可靠的服务。

## 1. 网关架构设计

### 核心组件

```ts
interface LLMGateway {
  // 文本生成
  chat(request: ChatRequest): Promise<ChatResponse>;

  // 结构化输出 (JSON)
  json<T>(task: TaskType, request: JsonRequest): Promise<T>;

  // 流式响应
  stream(request: ChatRequest): AsyncIterable<ChatChunk>;

  // 嵌入向量
  embed(request: EmbedRequest): Promise<EmbedResponse>;
}

interface ChatRequest {
  messages: Message[];
  model?: ModelId;
  temperature?: number;
  maxTokens?: number;
  stream?: boolean;
  metadata?: RequestMetadata;
}

interface JsonRequest {
  prompt: string;
  schema?: JsonSchema;
  examples?: any[];
  model?: ModelId;
  temperature?: number;
}

interface RequestMetadata {
  userId?: string;
  taskId?: string;
  priority?: RequestPriority;
  tags?: string[];
}
```

## 2. 模型配置与路由

### 支持的模型

```ts
enum ModelProvider {
  DEEPSEEK = "deepseek",
  OPENAI = "openai",
  ANTHROPIC = "anthropic",
  LOCAL = "local",
}

interface ModelConfig {
  id: ModelId;
  provider: ModelProvider;
  name: string;
  maxTokens: number;
  costPer1kTokens: {
    input: number;
    output: number;
  };
  capabilities: ModelCapability[];
  rateLimits: RateLimit;
  priority: number; // 用于故障转移
}

enum ModelCapability {
  CHAT = "chat",
  JSON_MODE = "json",
  FUNCTION_CALLING = "function_calling",
  VISION = "vision",
  EMBEDDING = "embedding",
}

// 模型配置示例
const MODEL_CONFIGS: Record<ModelId, ModelConfig> = {
  "deepseek-chat": {
    id: "deepseek-chat",
    provider: ModelProvider.DEEPSEEK,
    name: "DeepSeek Chat",
    maxTokens: 4096,
    costPer1kTokens: { input: 0.0014, output: 0.002 },
    capabilities: [ModelCapability.CHAT, ModelCapability.JSON_MODE],
    rateLimits: { requestsPerMinute: 60, tokensPerMinute: 60000 },
    priority: 1,
  },
  "gpt-4": {
    id: "gpt-4",
    provider: ModelProvider.OPENAI,
    name: "GPT-4",
    maxTokens: 8192,
    costPer1kTokens: { input: 0.03, output: 0.06 },
    capabilities: [
      ModelCapability.CHAT,
      ModelCapability.JSON_MODE,
      ModelCapability.FUNCTION_CALLING,
    ],
    rateLimits: { requestsPerMinute: 40, tokensPerMinute: 40000 },
    priority: 2,
  },
};
```

### 任务路由策略

```ts
enum TaskType {
  KEYWORD_DERIVE = "keyword-derive",
  CONTENT_SUMMARY = "content-summary",
  ENTITY_EXTRACT = "entity-extract",
  REPORT_GENERATE = "report-generate",
  CONTENT_ANALYZE = "content-analyze",
}

const TASK_MODEL_MAPPING: Record<TaskType, ModelRoutingConfig> = {
  [TaskType.KEYWORD_DERIVE]: {
    primary: "deepseek-chat",
    fallback: ["gpt-4"],
    temperature: 0.2,
    maxTokens: 512,
  },
  [TaskType.CONTENT_SUMMARY]: {
    primary: "deepseek-chat",
    fallback: ["gpt-4"],
    temperature: 0.3,
    maxTokens: 512,
  },
  [TaskType.REPORT_GENERATE]: {
    primary: "gpt-4",
    fallback: ["deepseek-chat"],
    temperature: 0.7,
    maxTokens: 2048,
  },
};

interface ModelRoutingConfig {
  primary: ModelId;
  fallback: ModelId[];
  temperature: number;
  maxTokens: number;
  requiresCapability?: ModelCapability[];
}
```

## 3. 请求处理与优化

### 缓存策略

```ts
interface CacheConfig {
  enabled: boolean;
  ttl: number; // 缓存时间 (秒)
  keyStrategy: CacheKeyStrategy;
  storage: CacheStorage;
}

enum CacheKeyStrategy {
  EXACT_MATCH = "exact", // 完全匹配
  SEMANTIC_HASH = "semantic", // 语义哈希
  CONTENT_HASH = "content", // 内容哈希
}

class LLMCache {
  async get(key: string): Promise<CachedResponse | null> {
    const cached = await this.storage.get(key);
    if (!cached || this.isExpired(cached)) return null;
    return cached.response;
  }

  async set(key: string, response: any, ttl?: number): Promise<void> {
    await this.storage.set(key, {
      response,
      createdAt: Date.now(),
      ttl: ttl ?? this.config.ttl,
    });
  }

  private generateCacheKey(request: ChatRequest): string {
    switch (this.config.keyStrategy) {
      case CacheKeyStrategy.EXACT_MATCH:
        return this.hashRequest(request);
      case CacheKeyStrategy.CONTENT_HASH:
        return this.hashContent(request.messages);
      default:
        return this.semanticHash(request.messages);
    }
  }
}
```

### 速率限制

```ts
interface RateLimit {
  requestsPerMinute: number;
  tokensPerMinute: number;
  burstLimit?: number;
}

class RateLimiter {
  private windows = new Map<string, TokenBucket>();

  async checkLimit(
    userId: string,
    model: ModelId,
    estimatedTokens: number
  ): Promise<RateLimitResult> {
    const key = `${userId}:${model}`;
    const bucket = this.getBucket(key, model);

    if (!bucket.consumeRequests(1)) {
      return { allowed: false, reason: "Request rate limit exceeded" };
    }

    if (!bucket.consumeTokens(estimatedTokens)) {
      return { allowed: false, reason: "Token rate limit exceeded" };
    }

    return { allowed: true };
  }

  private getBucket(key: string, model: ModelId): TokenBucket {
    if (!this.windows.has(key)) {
      const config = MODEL_CONFIGS[model].rateLimits;
      this.windows.set(key, new TokenBucket(config));
    }
    return this.windows.get(key)!;
  }
}
```

## 4. 成本监控与控制

### 成本计算

```ts
interface CostTracker {
  trackRequest(request: ChatRequest, response: ChatResponse): Promise<void>;
  getCosts(filters: CostFilter): Promise<CostSummary>;
  checkBudget(userId: string): Promise<BudgetStatus>;
}

interface TokenUsage {
  inputTokens: number;
  outputTokens: number;
  totalTokens: number;
}

interface CostSummary {
  totalCost: number;
  costByModel: Record<ModelId, number>;
  costByUser: Record<string, number>;
  costByTask: Record<TaskType, number>;
  tokenUsage: TokenUsage;
  timeRange: { from: Date; to: Date };
}

class CostCalculator {
  calculateCost(model: ModelId, usage: TokenUsage): number {
    const config = MODEL_CONFIGS[model];
    const inputCost = (usage.inputTokens / 1000) * config.costPer1kTokens.input;
    const outputCost =
      (usage.outputTokens / 1000) * config.costPer1kTokens.output;
    return inputCost + outputCost;
  }
}
```

### 预算控制

```ts
interface BudgetConfig {
  userId: string;
  monthlyLimit: number;
  dailyLimit?: number;
  alertThreshold: number; // 0-1, 触发告警的阈值
}

class BudgetManager {
  async checkBudget(
    userId: string,
    estimatedCost: number
  ): Promise<BudgetCheckResult> {
    const config = await this.getBudgetConfig(userId);
    const currentUsage = await this.getCurrentUsage(userId);

    if (currentUsage.monthly + estimatedCost > config.monthlyLimit) {
      return { allowed: false, reason: "Monthly budget exceeded" };
    }

    if (
      config.dailyLimit &&
      currentUsage.daily + estimatedCost > config.dailyLimit
    ) {
      return { allowed: false, reason: "Daily budget exceeded" };
    }

    const monthlyUsageRatio = currentUsage.monthly / config.monthlyLimit;
    if (monthlyUsageRatio >= config.alertThreshold) {
      await this.sendBudgetAlert(userId, monthlyUsageRatio);
    }

    return { allowed: true };
  }
}
```

## 5. 统一接口实现

### LLM Gateway 实现

```ts
export class LLMGateway implements ILLMGateway {
  constructor(
    private providers: Map<ModelProvider, ILLMProvider>,
    private cache: LLMCache,
    private rateLimiter: RateLimiter,
    private costTracker: CostTracker,
    private budgetManager: BudgetManager
  ) {}

  async chat(request: ChatRequest): Promise<ChatResponse> {
    // 1. 参数验证与标准化
    const normalizedRequest = this.normalizeRequest(request);

    // 2. 缓存检查
    const cacheKey = this.cache.generateCacheKey(normalizedRequest);
    const cached = await this.cache.get(cacheKey);
    if (cached) return cached;

    // 3. 模型路由
    const model = this.selectModel(normalizedRequest);

    // 4. 速率限制检查
    const rateLimit = await this.rateLimiter.checkLimit(
      normalizedRequest.metadata?.userId || "anonymous",
      model,
      this.estimateTokens(normalizedRequest)
    );
    if (!rateLimit.allowed) {
      throw new RateLimitError(rateLimit.reason);
    }

    // 5. 预算检查
    const estimatedCost = this.estimateCost(model, normalizedRequest);
    const budget = await this.budgetManager.checkBudget(
      normalizedRequest.metadata?.userId || "anonymous",
      estimatedCost
    );
    if (!budget.allowed) {
      throw new BudgetExceededError(budget.reason);
    }

    // 6. 实际调用
    const response = await this.executeWithFallback(normalizedRequest, model);

    // 7. 成本跟踪
    await this.costTracker.trackRequest(normalizedRequest, response);

    // 8. 缓存结果
    await this.cache.set(cacheKey, response);

    return response;
  }

  async json<T>(task: TaskType, request: JsonRequest): Promise<T> {
    const routingConfig = TASK_MODEL_MAPPING[task];

    const chatRequest: ChatRequest = {
      messages: [{ role: "user", content: request.prompt }],
      model: request.model || routingConfig.primary,
      temperature: request.temperature || routingConfig.temperature,
      maxTokens: routingConfig.maxTokens,
      metadata: { taskId: task, ...request.metadata },
    };

    const response = await this.chat(chatRequest);

    // JSON 解析与验证
    try {
      const parsed = JSON.parse(response.content);
      if (request.schema) {
        await this.validateSchema(parsed, request.schema);
      }
      return parsed as T;
    } catch (error) {
      throw new JSONParseError(
        `Failed to parse LLM response as JSON: ${error.message}`
      );
    }
  }
}
```

### 故障转移机制

```ts
class FailoverManager {
  async executeWithFallback(
    request: ChatRequest,
    primaryModel: ModelId
  ): Promise<ChatResponse> {
    const routingConfig = this.getRoutingConfig(request.metadata?.taskId);
    const models = [primaryModel, ...routingConfig.fallback];

    let lastError: Error | null = null;

    for (const model of models) {
      try {
        const provider = this.getProvider(model);
        if (!(await provider.isHealthy())) continue;

        const response = await provider.chat({
          ...request,
          model,
        });

        // 记录成功使用的模型
        await this.recordModelUsage(model, true);
        return response;
      } catch (error) {
        lastError = error;
        await this.recordModelUsage(model, false);

        // 如果是致命错误，不继续尝试
        if (this.isFatalError(error)) {
          break;
        }
      }
    }

    throw lastError || new Error("All models failed");
  }
}
```

## 6. 监控与日志

### 指标收集

```ts
interface LLMMetrics {
  totalRequests: number;
  successRate: number;
  averageLatency: number;
  totalCost: number;
  modelUsageDistribution: Record<ModelId, number>;
  errorsByType: Record<string, number>;
  cacheHitRate: number;
}

class MetricsCollector {
  recordRequest(
    model: ModelId,
    latency: number,
    success: boolean,
    cost: number,
    cached: boolean
  ): void {
    // 记录请求指标
    this.metrics.totalRequests++;
    this.metrics.totalCost += cost;

    if (success) {
      this.metrics.successCount++;
    }

    if (cached) {
      this.metrics.cacheHits++;
    }

    this.metrics.latencies.push(latency);
    this.metrics.modelUsage[model] = (this.metrics.modelUsage[model] || 0) + 1;
  }
}
```

### 告警系统

```ts
class AlertManager {
  checkAlerts(): void {
    const metrics = this.metricsCollector.getMetrics();

    // 成功率告警
    if (metrics.successRate < 0.95) {
      this.sendAlert("LOW_SUCCESS_RATE", { current: metrics.successRate });
    }

    // 延迟告警
    if (metrics.averageLatency > 10000) {
      this.sendAlert("HIGH_LATENCY", { current: metrics.averageLatency });
    }

    // 成本告警
    const dailyCost = this.getDailyCost();
    if (dailyCost > this.config.dailyCostThreshold) {
      this.sendAlert("HIGH_DAILY_COST", { current: dailyCost });
    }
  }
}
```

## 7. 开发约束

- 所有 AI 相关功能必须通过 LLM Gateway 调用
- 不允许直接调用外部 LLM API
- 新增任务类型必须在 TASK_MODEL_MAPPING 中配置
- 所有调用必须包含适当的 metadata
- 敏感信息调用前必须经过脱敏处理
- 生产环境必须启用成本监控和预算控制

## 8. 配置示例

### 环境变量

```bash
# LLM Gateway 配置
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL=3600
LLM_DEFAULT_MODEL=deepseek-chat
LLM_ENABLE_FALLBACK=true

# 成本控制
LLM_MONTHLY_BUDGET=1000
LLM_DAILY_BUDGET=50
LLM_ALERT_THRESHOLD=0.8

# 速率限制
LLM_RATE_LIMIT_ENABLED=true
LLM_DEFAULT_RPM=60
LLM_DEFAULT_TPM=60000
```

### 使用示例

```ts
// 在业务代码中使用
import { llmGateway } from "@oak/agents/llm-gateway";

// 关键词派生
const keywords = await llmGateway.json("keyword-derive", {
  prompt: `Generate related keywords for: ${baseKeyword}`,
  schema: KeywordDeriveSchema,
});

// 内容摘要
const summary = await llmGateway.json("content-summary", {
  prompt: `Summarize this content: ${content}`,
  schema: SummarySchema,
  temperature: 0.3,
});
```
